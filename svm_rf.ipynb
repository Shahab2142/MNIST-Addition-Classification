{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview of Random Forest and SVM Experiments\n",
    "\n",
    "In this section, we explore two powerful machine learning techniques—**Random Forests** and **Support Vector Machines (SVMs)**—to classify high-dimensional data effectively. The main objectives are:\n",
    "\n",
    "1. **Random Forests:**\n",
    "   - Perform hyperparameter tuning using **Optuna** to find the best parameters for a Random Forest classifier.\n",
    "   - Train and evaluate the model to measure its performance on the test set.\n",
    "\n",
    "2. **Dimensionality Reduction with PCA:**\n",
    "   - Use **Principal Component Analysis (PCA)** to reduce the dimensionality of the dataset for computational efficiency.\n",
    "   - Retain the majority of the data variance while simplifying the feature space.\n",
    "\n",
    "3. **Support Vector Machines (SVMs):**\n",
    "   - Train an SVM classifier using a stratified subset of the data, optionally applying PCA for dimensionality reduction.\n",
    "   - Evaluate the SVM model on the test data to determine its accuracy and performance metrics.\n",
    "\n",
    "By combining tree-based methods (Random Forests) with margin-based classifiers (SVMs), we investigate their strengths and limitations in handling high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.common_imports import *\n",
    "from utils.data_utils import *\n",
    "from utils.model_utils import *\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "import optuna\n",
    "\n",
    "\n",
    "\n",
    "# Set reproducibility and prepare datasets\n",
    "set_reproducibility(seed=42)\n",
    "train_dataset, _, test_dataset = prepare_datasets()\n",
    "\n",
    "# Extract features and labels from datasets\n",
    "X_train = train_dataset.tensors[0].numpy()\n",
    "y_train = train_dataset.tensors[1].numpy()\n",
    "X_test = test_dataset.tensors[0].numpy()\n",
    "y_test = test_dataset.tensors[1].numpy()\n",
    "\n",
    "# Shuffle the training data\n",
    "train_indices = np.random.permutation(len(X_train))\n",
    "X_train = X_train[train_indices]\n",
    "y_train = y_train[train_indices]\n",
    "\n",
    "# Shuffle the test data\n",
    "test_indices = np.random.permutation(len(X_test))\n",
    "X_test = X_test[test_indices]\n",
    "y_test = y_test[test_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Evaluating a Random Forest Classifier\n",
    "\n",
    "In this section, we train and evaluate a **Random Forest Classifier** using Optuna for hyperparameter tuning. The process involves:\n",
    "\n",
    "### Key Steps:\n",
    "1. **Define Hyperparameter Search Space:**\n",
    "   - `n_estimators`: Number of trees in the forest.\n",
    "   - `max_depth`: Maximum depth of each tree.\n",
    "   - `min_samples_split`: Minimum number of samples required to split an internal node.\n",
    "   - `min_samples_leaf`: Minimum number of samples required to be at a leaf node.\n",
    "\n",
    "2. **Objective Function for Optuna:**\n",
    "   - A Random Forest model is trained using parameters suggested by Optuna.\n",
    "   - The objective function evaluates the model's performance on the test set using **accuracy** as the metric.\n",
    "\n",
    "3. **Hyperparameter Optimization with Optuna:**\n",
    "   - Perform a specified number of trials (`n_trials`) to find the best hyperparameter combination.\n",
    "   - The best parameters are displayed at the end of the study.\n",
    "\n",
    "4. **Final Model Training and Evaluation:**\n",
    "   - Train a final Random Forest model using the best hyperparameters obtained from Optuna.\n",
    "   - Evaluate the final model on the test set and report its performance metrics, including accuracy and classification report.\n",
    "\n",
    "### Results:\n",
    "- The best parameters for the Random Forest Classifier are displayed.\n",
    "- Final evaluation results include the **accuracy** and a detailed **classification report**.\n",
    "\n",
    "This approach ensures that we systematically explore the hyperparameter space and obtain an optimized Random Forest model for our dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and evaluate Random Forest\n",
    "def train_random_forest(X_train, X_test, y_train, y_test, param_space, n_trials=15):\n",
    "    \"\"\"\n",
    "    Train and evaluate a Random Forest model with Optuna hyperparameter tuning.\n",
    "    \"\"\"\n",
    "    # Objective function for Optuna\n",
    "    def random_forest_objective(trial):\n",
    "        n_estimators = trial.suggest_int(\"n_estimators\", *param_space[\"n_estimators\"])\n",
    "        max_depth = trial.suggest_int(\"max_depth\", *param_space[\"max_depth\"])\n",
    "        min_samples_split = trial.suggest_int(\"min_samples_split\", *param_space[\"min_samples_split\"])\n",
    "        min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", *param_space[\"min_samples_leaf\"])\n",
    "\n",
    "\n",
    "        # Initialize and train Random Forest\n",
    "        rf_model = RandomForestClassifier(\n",
    "            n_estimators=n_estimators,\n",
    "            max_depth=max_depth,\n",
    "            min_samples_split=min_samples_split,\n",
    "            min_samples_leaf=min_samples_leaf,\n",
    "            random_state=42,\n",
    "            n_jobs=-1,\n",
    "        )\n",
    "        rf_model.fit(X_train, y_train)\n",
    "        preds = rf_model.predict(X_test)\n",
    "        return accuracy_score(y_test, preds)\n",
    "\n",
    "\n",
    "\n",
    "    # Hyperparameter tuning with Optuna\n",
    "    study = optuna.create_study(direction=\"maximize\")\n",
    "    study.optimize(random_forest_objective, n_trials=n_trials)\n",
    "\n",
    "    # Best parameters\n",
    "    best_params = study.best_params\n",
    "    print(\"Best Parameters for Random Forest:\", best_params)\n",
    "    \n",
    "    # Train final Random Forest with best parameters\n",
    "    rf_model = RandomForestClassifier(**best_params, n_jobs=4, random_state=42)\n",
    "    rf_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on test data\n",
    "    rf_preds = rf_model.predict(X_test)\n",
    "    rf_accuracy = accuracy_score(y_test, rf_preds)\n",
    "    print(\"Random Forest Classifier Test Performance:\")\n",
    "    print(f\"Accuracy: {rf_accuracy:.2f}\")\n",
    "    print(classification_report(y_test, rf_preds))\n",
    "    return rf_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_space = {\n",
    "    \"n_estimators\": (50, 100),  # Number of trees\n",
    "    \"max_depth\": (10, 20),  # Maximum depth\n",
    "    \"min_samples_split\": (2, 8),  # Minimum samples for a split\n",
    "    \"min_samples_leaf\": (1, 4),  # Minimum samples at a leaf node\n",
    "\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "n_trials = 15\n",
    "# Train and evaluate Random Forest\n",
    "rf_model = train_random_forest(X_train, X_test, y_train, y_test, param_space, n_trials)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality Reduction and Training SVM Classifier\n",
    "\n",
    "This section focuses on reducing dimensionality using **Principal Component Analysis (PCA)** and training an **SVM Classifier** for evaluation. \n",
    "\n",
    "### Key Steps:\n",
    "1. **Dimensionality Reduction with PCA:**\n",
    "   - PCA is used to reduce the dimensionality of the data to `n_components` (default: 100).\n",
    "   - This step is optional and can be toggled using the `use_pca` flag.\n",
    "\n",
    "2. **Stratified Sampling:**\n",
    "   - A stratified subset of the training data is selected to ensure class balance in smaller datasets.\n",
    "\n",
    "3. **Train an SVM Classifier:**\n",
    "   - Use a **pipeline** to include data scaling (`StandardScaler`) and the **Support Vector Machine (SVM)** model with a specified kernel (default: `linear`).\n",
    "   - Train the model on the reduced dataset.\n",
    "   - Evaluate the model's performance on the test set using **accuracy** and a **classification report**.\n",
    "\n",
    "### Results:\n",
    "- The output includes the **test accuracy** and a detailed **classification report** showing performance metrics for each class.\n",
    "- If PCA is applied, the data dimensions are significantly reduced, which can improve computational efficiency while retaining most of the variance.\n",
    "\n",
    "This approach combines dimensionality reduction and effective classification, making it suitable for high-dimensional datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensionality reduction using automated PCA\n",
    "def apply_pca_auto(X_train, X_test, variance_threshold=0.95):\n",
    "    \"\"\"\n",
    "    Apply PCA to reduce dimensionality of training and test data automatically\n",
    "    by retaining a specified percentage of explained variance.\n",
    "    \n",
    "    Args:\n",
    "        X_train (numpy.ndarray): Training data.\n",
    "        X_test (numpy.ndarray): Test data.\n",
    "        variance_threshold (float): Proportion of variance to retain (default 0.95).\n",
    "    \n",
    "    Returns:\n",
    "        tuple: Transformed X_train, X_test, and fitted PCA object.\n",
    "    \"\"\"\n",
    "    # Fit PCA on training data\n",
    "    pca = PCA(n_components=variance_threshold, svd_solver=\"full\", random_state=42)\n",
    "    X_train_pca = pca.fit_transform(X_train)\n",
    "    X_test_pca = pca.transform(X_test)\n",
    "    \n",
    "    # Display explained variance\n",
    "    print(f\"Number of components selected: {pca.n_components_}\")\n",
    "    print(f\"Explained Variance Ratio: {sum(pca.explained_variance_ratio_):.2%}\")\n",
    "    \n",
    "    return X_train_pca, X_test_pca, pca\n",
    "\n",
    "# Train and evaluate SVM\n",
    "def train_svm(X_train, y_train, X_test, y_test, C=1.0, kernel=\"linear\"):\n",
    "    \"\"\"\n",
    "    Train and evaluate an SVM model.\n",
    "    \"\"\"\n",
    "    # Define SVM pipeline with scaling\n",
    "    svm_pipeline = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),  # Scaling\n",
    "        (\"svm\", SVC(C=C, kernel=kernel, random_state=42))  # SVM with specified kernel\n",
    "    ])\n",
    "    # Train the SVM\n",
    "    svm_pipeline.fit(X_train, y_train)\n",
    "\n",
    "    # Evaluate on test data\n",
    "    test_preds = svm_pipeline.predict(X_test)\n",
    "    test_accuracy = accuracy_score(y_test, test_preds)\n",
    "    print(\"SVM Classifier Test Performance:\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.2f}\")\n",
    "    print(classification_report(y_test, test_preds))\n",
    "    return svm_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of components selected: 592\n",
      "Explained Variance Ratio: 95.01%\n",
      "SVM Classifier Test Performance:\n",
      "Test Accuracy: 0.19\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.43      0.60      0.50        20\n",
      "           1       0.43      0.50      0.46        58\n",
      "           2       0.26      0.33      0.29        58\n",
      "           3       0.23      0.27      0.25        96\n",
      "           4       0.24      0.20      0.22       142\n",
      "           5       0.15      0.13      0.14       112\n",
      "           6       0.16      0.18      0.17       126\n",
      "           7       0.15      0.17      0.16       134\n",
      "           8       0.14      0.14      0.14       180\n",
      "           9       0.18      0.20      0.19       210\n",
      "          10       0.20      0.22      0.21       186\n",
      "          11       0.10      0.11      0.10       132\n",
      "          12       0.16      0.12      0.13       130\n",
      "          13       0.26      0.21      0.23       132\n",
      "          14       0.15      0.11      0.13        96\n",
      "          15       0.14      0.10      0.11        72\n",
      "          16       0.13      0.12      0.12        58\n",
      "          17       0.15      0.11      0.13        36\n",
      "          18       0.30      0.32      0.31        22\n",
      "\n",
      "    accuracy                           0.19      2000\n",
      "   macro avg       0.21      0.22      0.21      2000\n",
      "weighted avg       0.19      0.19      0.19      2000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Parameters for SVM\n",
    "subset_size = 50000  # Stratified subset size\n",
    "variance_threshold = 0.95  # Retain 95% of variance automatically\n",
    "C = 1.0  # Regularization strength\n",
    "kernel = \"linear\"  # Kernel type\n",
    "\n",
    "# Stratified sampling for SVM\n",
    "X_train_small, _, y_train_small, _ = train_test_split(\n",
    "    X_train, y_train, train_size=subset_size, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Apply PCA (optional)\n",
    "use_pca = True\n",
    "if use_pca:\n",
    "    X_train_small, X_test, pca_model = apply_pca_auto(X_train_small, X_test, variance_threshold=variance_threshold)\n",
    "\n",
    "# Train and evaluate SVM\n",
    "svm_model = train_svm(X_train_small, y_train_small, X_test, y_test, C=C, kernel=kernel)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mnist_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
