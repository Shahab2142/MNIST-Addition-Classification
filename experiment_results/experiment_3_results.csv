activation_function,dropout_rate,val_loss,train_loss
ReLU,0.0,0.308748432032764,0.2322092309375604
ReLU,0.03,0.29130608692765236,0.34761617033084236
ReLU,0.06,0.2643855706034228,0.2566379263162613
ReLU,0.1,0.2716455178037286,0.3484793888648351
ReLU,0.15,0.24742109979689123,0.38524525416294736
ReLU,0.2,0.23631821145117282,0.42445284611781436
ReLU,0.25,0.2232291431427002,0.4837545404990514
ReLU,0.3,0.2931362096890807,0.5555549313545227
ReLU,0.35,0.2352770224660635,0.6147484244902929
ReLU,0.4,0.25193372908234596,0.6963804564555486
ReLU,0.5,0.3031438204646111,0.875230997244517
Tanh,0.0,0.5676202708482743,1.0050824445724487
Tanh,0.03,0.5841943210363388,1.1026667435646058
Tanh,0.06,0.5540617178678513,1.06403273007075
Tanh,0.1,0.5240852084159852,1.1308913179715474
Tanh,0.15,0.5881037304401397,1.2404536210695902
Tanh,0.2,0.5823171153068543,1.2848672541618347
Tanh,0.25,0.5928384063243866,1.355668253294627
Tanh,0.3,0.6512163629531861,1.4885236987113952
Tanh,0.35,0.7329278593063354,1.6011345324834187
Tanh,0.4,0.8343199405670166,1.7286874163309733
Tanh,0.5,1.121586651802063,1.9948426068623861
